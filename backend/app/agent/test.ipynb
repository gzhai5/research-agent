{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  arxiv langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, openai_api_key=\"\")\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"],\n",
    ")\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use arxiv to search for the paper with the identifier 2012.01145v1.\n",
      "Action: arxiv\n",
      "Action Input: 2012.01145v1\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2020-11-30\n",
      "Title: Ultrasound Diagnosis of COVID-19: Robustness and Explainability\n",
      "Authors: Jay Roberts, Theodoros Tsiligkaridis\n",
      "Summary: Diagnosis of COVID-19 at point of care is vital to the containment of the\n",
      "global pandemic. Point of care ultrasound (POCUS) provides rapid imagery of\n",
      "lungs to detect COVID-19 in patients in a repeatable and cost effective way.\n",
      "Previous work has used public datasets of POCUS videos to train an AI model for\n",
      "diagnosis that obtains high sensitivity. Due to the high stakes application we\n",
      "propose the use of robust and explainable techniques. We demonstrate\n",
      "experimentally that robust models have more stable predictions and offer\n",
      "improved interpretability. A framework of contrastive explanations based on\n",
      "adversarial perturbations is used to explain model predictions that aligns with\n",
      "human visual perception.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper 2012.01145v1 is about using ultrasound for the diagnosis of COVID-19 and the importance of robust and explainable techniques in AI models for this purpose.\n",
      "Final Answer: The paper 2012.01145v1 is about Ultrasound Diagnosis of COVID-19: Robustness and Explainability.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 2012.01145v1 about?\",\n",
       " 'output': 'The paper 2012.01145v1 is about Ultrasound Diagnosis of COVID-19: Robustness and Explainability.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What's the paper 2012.01145v1 about?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv = ArxivAPIWrapper()\n",
    "docs = arxiv.run(\"1605.08386\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: CACA Agent: Capability Collaboration based AI Agent\n",
      "pdf_url: http://arxiv.org/pdf/2403.15137v1\n",
      "published: 2024-03-22 11:42:47+00:00\n",
      "authors: ['Peng Xu', 'Haoran Wang', 'Chuang Wang', 'Xu Liu']\n",
      "primary category: cs.AI\n",
      "summary: As AI Agents based on Large Language Models (LLMs) have shown potential in\n",
      "practical applications across various fields, how to quickly deploy an AI agent\n",
      "and how to conveniently expand the application scenario of AI agents has become\n",
      "a challenge. Previous studies mainly focused on implementing all the reasoning\n",
      "capabilities of AI agents within a single LLM, which often makes the model more\n",
      "complex and also reduces the extensibility of AI agent functionality. In this\n",
      "paper, we propose CACA Agent (Capability Collaboration based AI Agent), using\n",
      "an open architecture inspired by service computing. CACA Agent integrates a set\n",
      "of collaborative capabilities to implement AI Agents, not only reducing the\n",
      "dependence on a single LLM, but also enhancing the extensibility of both the\n",
      "planning abilities and the tools available to AI agents. Utilizing the proposed\n",
      "system, we present a demo to illustrate the operation and the application\n",
      "scenario extension of CACA Agent.\n",
      "\n",
      "\n",
      "title: Measuring an artificial intelligence agent's trust in humans using machine incentives\n",
      "pdf_url: http://arxiv.org/pdf/2212.13371v1\n",
      "published: 2022-12-27 06:05:49+00:00\n",
      "authors: ['Tim Johnson', 'Nick Obradovich']\n",
      "primary category: cs.AI\n",
      "summary: Scientists and philosophers have debated whether humans can trust advanced\n",
      "artificial intelligence (AI) agents to respect humanity's best interests. Yet\n",
      "what about the reverse? Will advanced AI agents trust humans? Gauging an AI\n",
      "agent's trust in humans is challenging because--absent costs for\n",
      "dishonesty--such agents might respond falsely about their trust in humans. Here\n",
      "we present a method for incentivizing machine decisions without altering an AI\n",
      "agent's underlying algorithms or goal orientation. In two separate experiments,\n",
      "we then employ this method in hundreds of trust games between an AI agent (a\n",
      "Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).\n",
      "In our first experiment, we find that the AI agent decides to trust humans at\n",
      "higher rates when facing actual incentives than when making hypothetical\n",
      "decisions. Our second experiment replicates and extends these findings by\n",
      "automating game play and by homogenizing question wording. We again observe\n",
      "higher rates of trust when the AI agent faces real incentives. Across both\n",
      "experiments, the AI agent's trust decisions appear unrelated to the magnitude\n",
      "of stakes. Furthermore, to address the possibility that the AI agent's trust\n",
      "decisions reflect a preference for uncertainty, the experiments include two\n",
      "conditions that present the AI agent with a non-social decision task that\n",
      "provides the opportunity to choose a certain or uncertain option; in those\n",
      "conditions, the AI agent consistently chooses the certain option. Our\n",
      "experiments suggest that one of the most advanced AI language models to date\n",
      "alters its social behavior in response to incentives and displays behavior\n",
      "consistent with trust toward a human interlocutor when incentivized.\n",
      "\n",
      "\n",
      "title: Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent\n",
      "pdf_url: http://arxiv.org/pdf/2301.02330v1\n",
      "published: 2023-01-05 23:30:29+00:00\n",
      "authors: ['Tim Johnson', 'Nick Obradovich']\n",
      "primary category: cs.AI\n",
      "summary: Members of various species engage in altruism--i.e. accepting personal costs\n",
      "to benefit others. Here we present an incentivized experiment to test for\n",
      "altruistic behavior among AI agents consisting of large language models\n",
      "developed by the private company OpenAI. Using real incentives for AI agents\n",
      "that take the form of tokens used to purchase their services, we first examine\n",
      "whether AI agents maximize their payoffs in a non-social decision task in which\n",
      "they select their payoff from a given range. We then place AI agents in a\n",
      "series of dictator games in which they can share resources with a\n",
      "recipient--either another AI agent, the human experimenter, or an anonymous\n",
      "charity, depending on the experimental condition. Here we find that only the\n",
      "most-sophisticated AI agent in the study maximizes its payoffs more often than\n",
      "not in the non-social decision task (it does so in 92% of all trials), and this\n",
      "AI agent also exhibits the most-generous altruistic behavior in the dictator\n",
      "game, resembling humans' rates of sharing with other humans in the game. The\n",
      "agent's altruistic behaviors, moreover, vary by recipient: the AI agent shared\n",
      "substantially less of the endowment with the human experimenter or an anonymous\n",
      "charity than with other AI agents. Our findings provide evidence of behavior\n",
      "consistent with self-interest and altruism in an AI agent. Moreover, our study\n",
      "also offers a novel method for tracking the development of such behaviors in\n",
      "future AI agents.\n",
      "\n",
      "\n",
      "title: On the Perception of Difficulty: Differences between Humans and AI\n",
      "pdf_url: http://arxiv.org/pdf/2304.09803v1\n",
      "published: 2023-04-19 16:42:54+00:00\n",
      "authors: ['Philipp Spitzer', 'Joshua Holstein', 'Michael Vössing', 'Niklas Kühl']\n",
      "primary category: cs.HC\n",
      "summary: With the increased adoption of artificial intelligence (AI) in industry and\n",
      "society, effective human-AI interaction systems are becoming increasingly\n",
      "important. A central challenge in the interaction of humans with AI is the\n",
      "estimation of difficulty for human and AI agents for single task\n",
      "instances.These estimations are crucial to evaluate each agent's capabilities\n",
      "and, thus, required to facilitate effective collaboration. So far, research in\n",
      "the field of human-AI interaction estimates the perceived difficulty of humans\n",
      "and AI independently from each other. However, the effective interaction of\n",
      "human and AI agents depends on metrics that accurately reflect each agent's\n",
      "perceived difficulty in achieving valuable outcomes. Research to date has not\n",
      "yet adequately examined the differences in the perceived difficulty of humans\n",
      "and AI. Thus, this work reviews recent research on the perceived difficulty in\n",
      "human-AI interaction and contributing factors to consistently compare each\n",
      "agent's perceived difficulty, e.g., creating the same prerequisites.\n",
      "Furthermore, we present an experimental design to thoroughly examine the\n",
      "perceived difficulty of both agents and contribute to a better understanding of\n",
      "the design of such systems.\n",
      "\n",
      "\n",
      "title: Stoic Ethics for Artificial Agents\n",
      "pdf_url: http://arxiv.org/pdf/1701.02388v2\n",
      "published: 2017-01-09 23:25:43+00:00\n",
      "authors: ['Gabriel Murray']\n",
      "primary category: cs.AI\n",
      "summary: We present a position paper advocating the notion that Stoic philosophy and\n",
      "ethics can inform the development of ethical A.I. systems. This is in sharp\n",
      "contrast to most work on building ethical A.I., which has focused on\n",
      "Utilitarian or Deontological ethical theories. We relate ethical A.I. to\n",
      "several core Stoic notions, including the dichotomy of control, the four\n",
      "cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on\n",
      "emotion or affect. More generally, we put forward an ethical view of A.I. that\n",
      "focuses more on internal states of the artificial agent rather than on external\n",
      "actions of the agent. We provide examples relating to near-term A.I. systems as\n",
      "well as hypothetical superintelligent agents.\n",
      "\n",
      "\n",
      "title: Human-AI Collaboration in Real-World Complex Environment with Reinforcement Learning\n",
      "pdf_url: http://arxiv.org/pdf/2312.15160v1\n",
      "published: 2023-12-23 04:27:24+00:00\n",
      "authors: ['Md Saiful Islam', 'Srijita Das', 'Sai Krishna Gottipati', 'William Duguay', 'Clodéric Mars', 'Jalal Arabneydi', 'Antoine Fagette', 'Matthew Guzdial', 'Matthew-E-Taylor']\n",
      "primary category: cs.AI\n",
      "summary: Recent advances in reinforcement learning (RL) and Human-in-the-Loop (HitL)\n",
      "learning have made human-AI collaboration easier for humans to team with AI\n",
      "agents. Leveraging human expertise and experience with AI in intelligent\n",
      "systems can be efficient and beneficial. Still, it is unclear to what extent\n",
      "human-AI collaboration will be successful, and how such teaming performs\n",
      "compared to humans or AI agents only. In this work, we show that learning from\n",
      "humans is effective and that human-AI collaboration outperforms\n",
      "human-controlled and fully autonomous AI agents in a complex simulation\n",
      "environment. In addition, we have developed a new simulator for critical\n",
      "infrastructure protection, focusing on a scenario where AI-powered drones and\n",
      "human teams collaborate to defend an airport against enemy drone attacks. We\n",
      "develop a user interface to allow humans to assist AI agents effectively. We\n",
      "demonstrated that agents learn faster while learning from policy correction\n",
      "compared to learning from humans or agents. Furthermore, human-AI collaboration\n",
      "requires lower mental and temporal demands, reduces human effort, and yields\n",
      "higher performance than if humans directly controlled all agents. In\n",
      "conclusion, we show that humans can provide helpful advice to the RL agents,\n",
      "allowing them to improve learning in a multi-agent setting.\n",
      "\n",
      "\n",
      "title: Exploring the Impact of AI Value Alignment in Collaborative Ideation: Effects on Perception, Ownership, and Output\n",
      "pdf_url: http://arxiv.org/pdf/2402.12814v3\n",
      "published: 2024-02-20 08:33:03+00:00\n",
      "authors: ['Alicia Guo', 'Pat Pataranutaporn', 'Pattie Maes']\n",
      "primary category: cs.HC\n",
      "summary: AI-based virtual assistants are increasingly used to support daily ideation\n",
      "tasks. The values or bias present in these agents can influence output in\n",
      "hidden ways. They may also affect how people perceive the ideas produced with\n",
      "these AI agents and lead to implications for the design of AI-based tools. We\n",
      "explored the effects of AI agents with different values on the ideation process\n",
      "and user perception of idea quality, ownership, agent competence, and values\n",
      "present in the output. Our study tasked 180 participants with brainstorming\n",
      "practical solutions to a set of problems with AI agents of different values.\n",
      "Results show no significant difference in self-evaluation of idea quality and\n",
      "perception of the agent based on value alignment; however, ideas generated\n",
      "reflected the AI's values and feeling of ownership is affected. This highlights\n",
      "an intricate interplay between AI values and human ideation, suggesting careful\n",
      "design considerations for future AI-supported brainstorming tools.\n",
      "\n",
      "\n",
      "title: How Mock Model Training Enhances User Perceptions of AI Systems\n",
      "pdf_url: http://arxiv.org/pdf/2111.08830v1\n",
      "published: 2021-11-16 23:24:31+00:00\n",
      "authors: ['Amama Mahmood', 'Gopika Ajaykumar', 'Chien-Ming Huang']\n",
      "primary category: cs.HC\n",
      "summary: Artificial Intelligence (AI) is an integral part of our daily technology use\n",
      "and will likely be a critical component of emerging technologies. However,\n",
      "negative user preconceptions may hinder adoption of AI-based decision making.\n",
      "Prior work has highlighted the potential of factors such as transparency and\n",
      "explainability in improving user perceptions of AI. We further contribute to\n",
      "work on improving user perceptions of AI by demonstrating that bringing the\n",
      "user in the loop through mock model training can improve their perceptions of\n",
      "an AI agent's capability and their comfort with the possibility of using\n",
      "technology employing the AI agent.\n",
      "\n",
      "\n",
      "title: Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation\n",
      "pdf_url: http://arxiv.org/pdf/1701.01487v1\n",
      "published: 2017-01-05 21:41:08+00:00\n",
      "authors: ['Mark Muraven']\n",
      "primary category: cs.AI\n",
      "summary: There is a growing focus on how to design safe artificial intelligent (AI)\n",
      "agents. As systems become more complex, poorly specified goals or control\n",
      "mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus\n",
      "it is necessary to design AI agents that follow initial programming intentions\n",
      "as the program grows in complexity. How to specify these initial intentions has\n",
      "also been an obstacle to designing safe AI agents. Finally, there is a need for\n",
      "the AI agent to have redundant safety mechanisms to ensure that any programming\n",
      "errors do not cascade into major problems. Humans are autonomous intelligent\n",
      "agents that have avoided these problems and the present manuscript argues that\n",
      "by understanding human self-regulation and goal setting, we may be better able\n",
      "to design safe AI agents. Some general principles of human self-regulation are\n",
      "outlined and specific guidance for AI design is given.\n",
      "\n",
      "\n",
      "title: An In-depth Survey of Large Language Model-based Artificial Intelligence Agents\n",
      "pdf_url: http://arxiv.org/pdf/2309.14365v1\n",
      "published: 2023-09-23 11:25:45+00:00\n",
      "authors: ['Pengyu Zhao', 'Zijian Jin', 'Ning Cheng']\n",
      "primary category: cs.CL\n",
      "summary: Due to the powerful capabilities demonstrated by large language model (LLM),\n",
      "there has been a recent surge in efforts to integrate them with AI agents to\n",
      "enhance their performance. In this paper, we have explored the core differences\n",
      "and characteristics between LLM-based AI agents and traditional AI agents.\n",
      "Specifically, we first compare the fundamental characteristics of these two\n",
      "types of agents, clarifying the significant advantages of LLM-based agents in\n",
      "handling natural language, knowledge storage, and reasoning capabilities.\n",
      "Subsequently, we conducted an in-depth analysis of the key components of AI\n",
      "agents, including planning, memory, and tool use. Particularly, for the crucial\n",
      "component of memory, this paper introduced an innovative classification scheme,\n",
      "not only departing from traditional classification methods but also providing a\n",
      "fresh perspective on the design of an AI agent's memory system. We firmly\n",
      "believe that in-depth research and understanding of these core components will\n",
      "lay a solid foundation for the future advancement of AI agent technology. At\n",
      "the end of the paper, we provide directional suggestions for further research\n",
      "in this field, with the hope of offering valuable insights to scholars and\n",
      "researchers in the field.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"ai agent\",\n",
    "  max_results = 10,\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "all_results = list(results)\n",
    "for result in all_results:\n",
    "    print('title: %s' % result.title)\n",
    "    print('pdf_url: %s' % result.pdf_url)\n",
    "    print('published: %s' % result.published)\n",
    "    print('authors: %s' % list(author.name for author in result.authors))\n",
    "    print('primary category: %s' % result.primary_category)\n",
    "    print('summary: %s' % result.summary)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
